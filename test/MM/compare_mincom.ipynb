{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "committed-spell",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import random\n",
    "from scipy.io import mmread\n",
    "import math\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.spatial.distance import cosine\n",
    "import os\n",
    "from matplotlib import gridspec\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.feature_selection import SelectKBest, SelectFromModel\n",
    "from sklearn.feature_selection import chi2,f_classif,mutual_info_classif,mutual_info_regression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import tree\n",
    "from scipy.stats import chisquare\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from parfor import parfor\n",
    "import time\n",
    "\n",
    "from parfor import parfor\n",
    "from scipy import io as sio\n",
    "from scipy import sparse as ss\n",
    "from scipy import optimize as so\n",
    "from sklearn.utils.sparsefuncs import mean_variance_axis\n",
    "from scipy.stats import linregress\n",
    "import csv\n",
    "import seaborn as sns\n",
    "import copy\n",
    "from sklearn.cluster import KMeans\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_genes(genes):\n",
    "    '''\n",
    "    Load the gene names from a file\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    genes : str\n",
    "        Path to a gene file\n",
    "    '''\n",
    "    try:\n",
    "        genes = np.array([row[1].upper() for row in csv.reader(open(genes), delimiter=\"\\t\")]) # 10X format\n",
    "    except:\n",
    "        genes = np.array([row[0].upper() for row in csv.reader(open(genes), delimiter=\"\\t\")]) # base format with one gene name per row\n",
    "    return genes\n",
    "\n",
    "## Load in the data and create a metadata table\n",
    "def load_multiple_samples(samples, barcodes, genefile):\n",
    "    '''\n",
    "    Load the gene names from a set of mtx files\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    samples : dict\n",
    "        dictionary of sample names and path to matrix file\n",
    "    barcodes : dict\n",
    "        dictionary of sample names and path to barcodes file\n",
    "    genefile : str\n",
    "        Path to a gene file\n",
    "    '''\n",
    "\n",
    "    genes = load_genes(genefile)\n",
    "\n",
    "    samplenames = list(samples.keys())\n",
    "    sampleorder = list(np.sort(samplenames))\n",
    "\n",
    "    Mlist = []\n",
    "    meta=pd.DataFrame()\n",
    "    for x in sampleorder: \n",
    "        print('Loading in sample: ' + x)\n",
    "        currM = sio.mmread(samples[x]).tocsc()\n",
    "        currbc = list(pd.read_csv(barcodes[x], header=None)[0])\n",
    "        Mlist.append(currM)\n",
    "        currmeta = pd.DataFrame({'sample':[x]*currM.shape[1]})\n",
    "        currmeta.index = [currbc[i] + '_' + x for i in range(len(currbc))]  \n",
    "        meta = meta.append(currmeta)\n",
    "\n",
    "    M = ss.hstack(Mlist)\n",
    "\n",
    "    return M, genes, meta\n",
    "\n",
    "def load_consolidated_data(matrixfile, metafile, genefile):\n",
    "\n",
    "    '''\n",
    "    Load the gene names from a set of mtx files\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    matrixfile : string\n",
    "        path to matrix file\n",
    "    metafile : string\n",
    "        path to metadata file \n",
    "    genefile : str\n",
    "        Path to a gene file\n",
    "    '''\n",
    "    genes = load_genes(genefile)\n",
    "    meta = pd.read_csv(metafile, header=0) \n",
    "    M = sio.mmread(matrixfile).tocsc()    \n",
    "    return M, genes, meta\n",
    "\n",
    "def normalization(M, meta):\n",
    "    '''\n",
    "    Normalize databy dividing by column sum\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    M : sparse matrix\n",
    "        gene expression matrix\n",
    "    meta : table\n",
    "        metadata table\n",
    "    '''\n",
    "\n",
    "    # normalize by dividing by column sum\n",
    "    M2 = copy.deepcopy(M)\n",
    "    sums = np.array(M2.sum(axis=0)).flatten() # compute sums of all columns (cells)\n",
    "    M2.data = M2.data.astype(float) # convert type from int to float prior to division\n",
    "\n",
    "    for i in range(len(M2.indptr)-1): # for each column i\n",
    "        rr = range(M2.indptr[i], M2.indptr[i+1]) # get range rr\n",
    "        M2.data[rr] = M2.data[rr]/sums[i] # divide data values by matching column sum\n",
    "\n",
    "    # add transcript totals to the metadata table\n",
    "    meta['transcript_total'] = sums\n",
    "\n",
    "    return M2, meta\n",
    "\n",
    "def filter_genes(M, offset):\n",
    "    '''\n",
    "    Find indices of genes which have suprapoisson coefficient of variation\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    M : array\n",
    "        genes-cells matrix\n",
    "    offset : float (between 1 and 1.3)\n",
    "        log10(offset) is added to the regression line when filtering genes. \n",
    "        Genes above the line are kept\n",
    "        Values between 1 (no offset) and 1.3 work well\n",
    "\n",
    "    '''\n",
    "    # select genes above the line \n",
    "    mean, var = mean_variance_axis(M, axis=1) # get genes means and variances from M\n",
    "    std = np.sqrt(var) # compute standard deviations from variances\n",
    "    cv = np.divide(std, mean, out=np.zeros_like(std), where=mean!=0) # compute coefficient of variation\n",
    "\n",
    "    # indices of genes that are present in more than .1% of the cells\n",
    "    MCSR = M.tocsr() # MCSR is the row oriented version of M\n",
    "    presence = np.array([MCSR.indptr[i+1]-MCSR.indptr[i] for i in range(M.shape[0])]) # count how many cells have non zeros expression for each gene\n",
    "    presence_idx = np.where(presence>M.shape[1]*0.001)[0] # get indices of genes that are expressed in more than .1% of the cells\n",
    "    MCSR = None\n",
    "\n",
    "    nzidx = np.nonzero(mean)[0] # indices of genes with non-zero mean\n",
    "    nzidx = np.intersect1d(nzidx, presence_idx) # get intersection of genes with non-zero mean and genes present in more than .1% of the cells\n",
    "\n",
    "    nzcv = cv[nzidx] # select the matching cvs\n",
    "    nzmean = mean[nzidx] # select the matching means\n",
    "    lognzcv = np.log10(nzcv) # log10\n",
    "    lognzmean = np.log10(nzmean) # log10\n",
    "\n",
    "    slope, intercept, r_value, p_value, std_err = linregress(lognzmean, lognzcv)\n",
    "\n",
    "    adjusted_intercept = intercept+np.log10(offset) # slide filtering line with offset\n",
    "    selection_idx = np.where(lognzcv>lognzmean*slope+adjusted_intercept)[0] # get indices of genes above the filtering line\n",
    "    final_idx = nzidx[selection_idx]\n",
    "\n",
    "    # Plot the data just to see the selected genes\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.scatter(lognzmean, lognzcv, s=1) # plot all genes\n",
    "    plt.scatter(lognzmean[selection_idx], lognzcv[selection_idx], c='maroon', s=1) # plot genes above line with different color\n",
    "    plt.plot(lognzmean,lognzmean*slope+adjusted_intercept, c='darkorange') # plot filtering line\n",
    "    plt.xlabel('log10(mean)')\n",
    "    plt.ylabel('log10(cv)')\n",
    "    plt.title('%d genes selected' % len(selection_idx))\n",
    "\n",
    "    return final_idx\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adequate-belarus",
   "metadata": {},
   "source": [
    "## Six comparing Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "early-auckland",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_create(path, name, msg):\n",
    "    full_path = path + \"/\" + name + '.txt'\n",
    "    file = open(full_path, 'w')\n",
    "    file.write(str(msg))\n",
    "\n",
    "def SVM(X, y):\n",
    "    model = svm.LinearSVC(max_iter=1000000)\n",
    "    model.fit(X, y)\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_error(model, X, y):\n",
    "    y_pred = model.predict(X)\n",
    "    return mean_squared_error(y_pred, y)\n",
    "\n",
    "def feature_compare(path,X, Y, X_test,Y_test,num_features, num_samples):\n",
    "    train_error=[]\n",
    "    test_error=[]\n",
    "    train_acc=[]\n",
    "    test_acc=[]\n",
    "    D = np.shape(X)[1]\n",
    "    # Random\n",
    "    ran_idx = random.choices(range(D), k=num_features)\n",
    "    text_create(path,'ran_idx',ran_idx)\n",
    "    for i in range(num_features):\n",
    "        model=SVM(X[:,ran_idx[:(i+1)]],Y)\n",
    "        train_error.append(get_error(model,X[:,ran_idx[:(i+1)]],Y))\n",
    "        train_acc.append(model.score(X[:,ran_idx[:(i+1)]],Y))\n",
    "        model=SVM(X_test[:,ran_idx[:(i+1)]],Y_test)\n",
    "        test_error.append(get_error(model,X_test[:,ran_idx[:(i+1)]],Y_test))\n",
    "        test_acc.append(model.score(X_test[:,ran_idx[:(i+1)]],Y_test))\n",
    "    print('ran_idx:')\n",
    "    print(ran_idx)\n",
    "    \n",
    "    # correlation coefficient \n",
    "    @parfor(range(num_features), bar=False)\n",
    "    def tmp(i):\n",
    "        indices=np.arange(np.shape(X)[0])\n",
    "        random.shuffle(indices)\n",
    "        model = SelectKBest(f_classif, k=D).fit(X[indices[:num_samples]], Y[indices[:num_samples]])\n",
    "        score_raw = np.abs(model.scores_).tolist()\n",
    "        score = [float('-inf') if math.isnan(x) or x==float('inf') else x for x in score_raw]\n",
    "        idx = model.get_support(indices=True).tolist()\n",
    "        return [x for _, x in sorted(zip(score, idx), reverse=True)]\n",
    "    cor_idx=[]\n",
    "    for i in range(num_features):\n",
    "        new_idx=[i for i in tmp[i] if i not in cor_idx][0]\n",
    "        cor_idx.append(new_idx)\n",
    "    text_create(path,'cor_idx',cor_idx)\n",
    "    for i in range(num_features):\n",
    "        model=SVM(X[:,cor_idx[:(i+1)]],Y)\n",
    "        train_error.append(get_error(model,X[:,cor_idx[:(i+1)]],Y))\n",
    "        train_acc.append(model.score(X[:,cor_idx[:(i+1)]],Y))\n",
    "        model=SVM(X_test[:,cor_idx[:(i+1)]],Y_test)\n",
    "        test_error.append(get_error(model,X_test[:,cor_idx[:(i+1)]],Y_test))\n",
    "        test_acc.append(model.score(X_test[:,cor_idx[:(i+1)]],Y_test))\n",
    "    print('cor_idx:')\n",
    "    print(cor_idx)\n",
    "    \n",
    "    # mutual\n",
    "    #@parfor(range(num_features), bar=False)\n",
    "    #def tmp(i):\n",
    "    tmp=[]\n",
    "    for i in range(num_features):\n",
    "        indices=np.arange(np.shape(X)[0])\n",
    "        random.shuffle(indices)\n",
    "        model = SelectKBest(mutual_info_classif, k=D).fit(X[indices[:num_samples]], Y[indices[:num_samples]])\n",
    "        score_raw = np.abs(model.scores_).tolist()\n",
    "        score = [float('-inf') if math.isnan(x) or x==float('inf') else x for x in score_raw]\n",
    "        idx = model.get_support(indices=True).tolist()\n",
    "        tmp.append([x for _, x in sorted(zip(score, idx), reverse=True)])\n",
    "        #return [x for _, x in sorted(zip(score, idx), reverse=True)]\n",
    "    mut_idx=[]\n",
    "    for i in range(num_features):\n",
    "        new_idx=[i for i in tmp[i] if i not in mut_idx][0]\n",
    "        mut_idx.append(new_idx)\n",
    "    text_create(path,'mut_idx',mut_idx)\n",
    "    for i in range(num_features):\n",
    "        model=SVM(X[:,mut_idx[:(i+1)]],Y)\n",
    "        train_error.append(get_error(model,X[:,mut_idx[:(i+1)]],Y))\n",
    "        train_acc.append(model.score(X[:,mut_idx[:(i+1)]],Y))\n",
    "        model=SVM(X_test[:,mut_idx[:(i+1)]],Y_test)\n",
    "        test_error.append(get_error(model,X_test[:,mut_idx[:(i+1)]],Y_test))\n",
    "        test_acc.append(model.score(X_test[:,mut_idx[:(i+1)]],Y_test))\n",
    "    print('mut_idx:')\n",
    "    print(mut_idx)\n",
    "    \n",
    "    # chi2\n",
    "    @parfor(range(num_features), bar=False)\n",
    "    def tmp(i):\n",
    "        indices=np.arange(np.shape(X)[0])\n",
    "        random.shuffle(indices)\n",
    "        model = SelectKBest(chi2, k=D).fit(X[indices[:num_samples]], Y[indices[:num_samples]])\n",
    "        score_raw = np.abs(model.scores_).tolist()\n",
    "        score = [float('-inf') if math.isnan(x) or x==float('inf') else x for x in score_raw]\n",
    "        idx = model.get_support(indices=True).tolist()\n",
    "        return [x for _, x in sorted(zip(score, idx), reverse=True)]\n",
    "    chi_idx=[]\n",
    "    for i in range(num_features):\n",
    "        new_idx=[i for i in tmp[i] if i not in chi_idx][0]\n",
    "        chi_idx.append(new_idx)\n",
    "    text_create(path,'chi_idx',chi_idx)\n",
    "    for i in range(num_features):\n",
    "        model=SVM(X[:,chi_idx[:(i+1)]],Y)\n",
    "        train_error.append(get_error(model,X[:,chi_idx[:(i+1)]],Y))\n",
    "        train_acc.append(model.score(X[:,chi_idx[:(i+1)]],Y))\n",
    "        model=SVM(X_test[:,chi_idx[:(i+1)]],Y_test)\n",
    "        test_error.append(get_error(model,X_test[:,chi_idx[:(i+1)]],Y_test))\n",
    "        test_acc.append(model.score(X_test[:,chi_idx[:(i+1)]],Y_test))\n",
    "    print('chi_idx:')\n",
    "    print(chi_idx)\n",
    "    \n",
    "    # Feature importance is an inbuilt class that comes with Tree Based Classifiers\n",
    "    @parfor(range(num_features), bar=False)\n",
    "    def tmp(i):\n",
    "        indices=np.arange(np.shape(X)[0])\n",
    "        random.shuffle(indices)\n",
    "        model = ExtraTreesClassifier()\n",
    "        model.fit(X[indices[:num_samples]], Y[indices[:num_samples]])\n",
    "        coef = [float('-inf') if math.isnan(x) or x==float('inf') else x for x in np.abs(model.feature_importances_)]\n",
    "        return sorted(range(len(coef)), key=lambda i: coef[i], reverse=True)\n",
    "    tree_idx=[]\n",
    "    for i in range(num_features):\n",
    "        new_idx=[i for i in tmp[i] if i not in tree_idx][0]\n",
    "        tree_idx.append(new_idx)\n",
    "    text_create(path,'tree_idx',tree_idx)\n",
    "    for i in range(num_features):\n",
    "        model=SVM(X[:,tree_idx[:(i+1)]],Y)\n",
    "        train_error.append(get_error(model,X[:,tree_idx[:(i+1)]],Y))\n",
    "        train_acc.append(model.score(X[:,tree_idx[:(i+1)]],Y))\n",
    "        model=SVM(X_test[:,tree_idx[:(i+1)]],Y_test)\n",
    "        test_error.append(get_error(model,X_test[:,tree_idx[:(i+1)]],Y_test))\n",
    "        test_acc.append(model.score(X_test[:,tree_idx[:(i+1)]],Y_test))\n",
    "    print('tree_idx:')\n",
    "    print(tree_idx)\n",
    "    \n",
    "    # SVM by weight\n",
    "    @parfor(range(num_features), bar=False)\n",
    "    def tmp(i):\n",
    "        indices=np.arange(np.shape(X)[0])\n",
    "        random.shuffle(indices)\n",
    "        model=SVM(X[indices[:num_samples]], Y[indices[:num_samples]])\n",
    "        weight=np.sum(np.abs(model.coef_),axis=0).tolist()\n",
    "        coef = [0 if math.isnan(x) or x==float('inf') else x for x in weight]\n",
    "        return sorted(range(len(coef)), key=lambda i: coef[i], reverse=True)\n",
    "    svm_idx=[]\n",
    "    for i in range(num_features):\n",
    "        new_idx=[i for i in tmp[i] if i not in svm_idx][0]\n",
    "        svm_idx.append(new_idx)\n",
    "    text_create(path,'svm_idx',svm_idx)\n",
    "    for i in range(num_features):\n",
    "        model=SVM(X[:,svm_idx[:(i+1)]],Y)\n",
    "        train_error.append(get_error(model,X[:,svm_idx[:(i+1)]],Y))\n",
    "        train_acc.append(model.score(X[:,svm_idx[:(i+1)]],Y))\n",
    "        model=SVM(X_test[:,svm_idx[:(i+1)]],Y_test)\n",
    "        test_error.append(get_error(model,X_test[:,svm_idx[:(i+1)]],Y_test))\n",
    "        test_acc.append(model.score(X_test[:,svm_idx[:(i+1)]],Y_test))\n",
    "    print('svm_idx:')\n",
    "    print(svm_idx)\n",
    "    text_create(path,'compare_train_error',train_error)\n",
    "    text_create(path,'compare_test_error',test_error)\n",
    "    text_create(path,'compare_train_acc',train_acc)\n",
    "    text_create(path,'compare_test_acc',test_acc)\n",
    "    return train_error,test_error,train_acc,test_acc,ran_idx,cor_idx,mut_idx,chi_idx,tree_idx,svm_idx\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "northern-print",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "psychological-wallpaper",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file='./data/MM_all_matrix.mtx'\n",
    "gene_file='./data/genes.tsv'\n",
    "label_file='./data/MM_all_metadata.csv'\n",
    "\n",
    "M, raw_gene, meta = load_consolidated_data(data_file, label_file, gene_file)\n",
    "M2, meta = normalization(M, meta)\n",
    "meta = pd.read_csv(label_file,header=0)\n",
    "\n",
    "l=[]\n",
    "for i in range(np.shape(raw_gene)[0]):\n",
    "    if raw_gene[i][:3]!='RPL' and raw_gene[i][:3]!='RPS' and raw_gene[i][:2]!='MT':\n",
    "        l.append(i)\n",
    "gene=raw_gene[l]\n",
    "\n",
    "data=normalize(M2.tocsr().transpose()[:,l],axis=1, norm='l2')\n",
    "\n",
    "raw_target=meta[\"disease\"].values\n",
    "target=(raw_target=='MM')*1.0\n",
    "target=target.astype(np.uint8)\n",
    "classes=range(len(np.unique(target)))\n",
    "keys=['healthy','MM']\n",
    "del M,M2,raw_gene,meta,raw_target\n",
    "\n",
    "idx = np.arange(np.shape(data)[0])\n",
    "random.shuffle(idx)\n",
    "X_train = data[idx[:int(np.shape(data)[0]*4/5)],:]\n",
    "y_train = target[idx[:int(np.shape(data)[0]*4/5)]]\n",
    "X_test = data[idx[int(np.shape(data)[0]*4/5):],:]\n",
    "y_test = target[idx[int(np.shape(data)[0]*4/5):]]\n",
    "\n",
    "print(type(data))\n",
    "print(np.shape(data),np.shape(target),len(np.unique(target)))\n",
    "print(np.shape(X_train))\n",
    "print(np.shape(X_test))\n",
    "for i in np.unique(target):\n",
    "    print('class '+keys[i]+': '+str(np.count_nonzero((target==i)*1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consolidated-profession",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Parameters\n",
    "----------\n",
    "balance : boolean\n",
    "    balance the number of cells of each class or just randomly select cells at each loop\n",
    "num_features : int\n",
    "    the total number of genes we want to select \n",
    "num_samples : int\n",
    "    the number of cells we would use at each loop\n",
    "'''\n",
    "\n",
    "num_features = 150\n",
    "num_samples=20\n",
    "\n",
    "path='results/compare_mincom'\n",
    "try:\n",
    "    os.mkdir('results')\n",
    "except OSError:\n",
    "    print (\"Creation of the directory %s failed\")\n",
    "else:\n",
    "    print (\"Successfully created the directory %s \")\n",
    "try:\n",
    "    os.mkdir(path)\n",
    "except OSError:\n",
    "    print (\"Creation of the directory %s failed\" % path)\n",
    "else:\n",
    "    print (\"Successfully created the directory %s \" % path)\n",
    "\n",
    "train_error,test_error,train_acc,train_acc,ran_idx,fvalue_idx,mut_idx,chi_idx,tree_idx,svm_idx=feature_compare(path,X_train, y_train, X_test,y_test,num_features, num_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convinced-journey",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
