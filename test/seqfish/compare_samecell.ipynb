{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "committed-spell",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import csv\n",
    "from scipy import io as sio\n",
    "import copy\n",
    "from scipy.sparse import vstack\n",
    "\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import random\n",
    "from scipy.io import mmread\n",
    "import math\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.spatial.distance import cosine\n",
    "import os\n",
    "from matplotlib import gridspec\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.feature_selection import SelectKBest, SelectFromModel\n",
    "from sklearn.feature_selection import chi2,f_classif,mutual_info_classif,mutual_info_regression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import tree\n",
    "from scipy.stats import chisquare\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import time\n",
    "import time\n",
    "from scipy import io as sio\n",
    "from scipy import sparse as ss\n",
    "from scipy import optimize as so\n",
    "from sklearn.utils.sparsefuncs import mean_variance_axis\n",
    "from scipy.stats import linregress\n",
    "import csv\n",
    "import seaborn as sns\n",
    "import copy\n",
    "from sklearn.cluster import KMeans\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import normalize \n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "from matplotlib import gridspec\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from scipy.sparse import vstack\n",
    "\n",
    "import os, psutil\n",
    "import resource\n",
    "import scanpy as sc\n",
    "from sklearn.model_selection import KFold,GridSearchCV\n",
    "import mat73\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning) \n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adequate-belarus",
   "metadata": {},
   "source": [
    "## Six comparing Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "early-auckland",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimerError(Exception):\n",
    "     \"\"\"A custom exception used to report errors in use of Timer class\"\"\"\n",
    "\n",
    "class Timer:\n",
    "    def __init__(self):\n",
    "        self._start_time = None\n",
    "\n",
    "    def start(self):\n",
    "        if self._start_time is not None:\n",
    "            raise TimerError(f\"Timer is running. Use .stop() to stop it\")\n",
    "\n",
    "        self._start_time = time.perf_counter()\n",
    "\n",
    "    def stop(self):\n",
    "        if self._start_time is None:\n",
    "            raise TimerError(f\"Timer is not running. Use .start() to start it\")\n",
    "\n",
    "        elapsed_time = time.perf_counter() - self._start_time\n",
    "        self._start_time = None\n",
    "        #print(f\"Elapsed time: {elapsed_time:0.4f} seconds\")\n",
    "        return elapsed_time\n",
    "        \n",
    "def text_create(path, name, msg):\n",
    "    full_path = path + \"/\" + name + '.pickle'\n",
    "    f=open(full_path,'wb') \n",
    "    pickle.dump(msg,f)\n",
    "    f.close()\n",
    "    \n",
    "def SVM(X, y,class_weight=None):\n",
    "    model = svm.LinearSVC(loss='squared_hinge',tol=1, C=10,class_weight=class_weight, max_iter=1000)\n",
    "    model.fit(X, y)\n",
    "    return model\n",
    "\n",
    "def get_error(model, X, y,sample_weight=None):\n",
    "    y_pred = model.predict(X)\n",
    "    return mean_squared_error(y_pred, y,sample_weight=sample_weight)\n",
    "\n",
    "\n",
    "def compare_mincom_samples(path, memorys, X, Y, X_test,Y_test,num_features,init_samples,num_samples,class_weight=None):\n",
    "    train_error=[]\n",
    "    test_error=[]\n",
    "    train_acc=[]\n",
    "    test_acc=[]\n",
    "    step_times=[]\n",
    "    elapsed_times=[]\n",
    "    D = np.shape(X)[1]\n",
    "    N = np.shape(X)[0]\n",
    "    \n",
    "    if class_weight=='balanced':\n",
    "        classes, inverse, count=np.unique(Y,return_inverse=True, return_counts=True)\n",
    "        train_sample_weight=(Y.shape[0]/(len(classes)*count))[inverse]\n",
    "        classes, inverse, count=np.unique(Y_test,return_inverse=True, return_counts=True)\n",
    "        test_sample_weight=(Y_test.shape[0]/(len(classes)*count))[inverse]\n",
    "    else:\n",
    "        train_sample_weight=None\n",
    "        test_sample_weight=None\n",
    "    class1=[np.argwhere(Y==1)[0][0]]\n",
    "    ind_all=[]\n",
    "    ind_all.append(random.sample(range(N),init_samples-1)+class1)\n",
    "    for i in range(num_features-1):\n",
    "        ind_all.append(random.sample(range(N),num_samples-1)+class1)\n",
    "\n",
    "    # Random\n",
    "    memorys.append(psutil.Process(os.getpid()).memory_info().rss / 1024 ** 2)\n",
    "    memorys.append(resource.getrusage(resource.RUSAGE_SELF).ru_maxrss/ 1024 ** 2)\n",
    "    et=Timer()\n",
    "    et.start()\n",
    "    ran_idx = random.sample(range(D), num_features)\n",
    "    for i in range(num_features):\n",
    "        t=Timer()\n",
    "        t.start()\n",
    "        ind=ind_all[i]\n",
    "        model=SVM(X[:,ran_idx[:(i+1)]],Y,class_weight=class_weight)\n",
    "        train_error.append(get_error(model,X[:,ran_idx[:(i+1)]],Y,sample_weight=train_sample_weight))\n",
    "        train_acc.append(model.score(X[:,ran_idx[:(i+1)]],Y,sample_weight=train_sample_weight))\n",
    "        test_error.append(get_error(model,X_test[:,ran_idx[:(i+1)]],Y_test,sample_weight=test_sample_weight))\n",
    "        test_acc.append(model.score(X_test[:,ran_idx[:(i+1)]],Y_test,sample_weight=test_sample_weight))\n",
    "        step_times.append(t.stop())\n",
    "    elapsed_times.append(et.stop())\n",
    "    print('ran_idx')\n",
    "    \n",
    "    # correlation coefficient\n",
    "    memorys.append(psutil.Process(os.getpid()).memory_info().rss / 1024 ** 2)\n",
    "    memorys.append(resource.getrusage(resource.RUSAGE_SELF).ru_maxrss/ 1024 ** 2)\n",
    "    et=Timer()\n",
    "    et.start()\n",
    "    cor_idx=[]\n",
    "    for i in range(num_features):\n",
    "        t=Timer()\n",
    "        t.start()\n",
    "        ind=ind_all[i]\n",
    "        model = SelectKBest(f_classif, k=(i+1)).fit(X[ind],Y[ind])\n",
    "        tmp=model.get_support(indices=True).tolist()\n",
    "        cor_idx.append(tmp)\n",
    "        model=SVM(X[:,tmp],Y,class_weight=class_weight)\n",
    "        train_error.append(get_error(model,X[:,tmp],Y,sample_weight=train_sample_weight))\n",
    "        train_acc.append(model.score(X[:,tmp],Y,sample_weight=train_sample_weight))\n",
    "        test_error.append(get_error(model,X_test[:,tmp],Y_test,sample_weight=test_sample_weight))\n",
    "        test_acc.append(model.score(X_test[:,tmp],Y_test,sample_weight=test_sample_weight))\n",
    "        step_times.append(t.stop())\n",
    "    elapsed_times.append(et.stop())\n",
    "    print('cor_idx')\n",
    "    \n",
    "    # mutual\n",
    "    memorys.append(psutil.Process(os.getpid()).memory_info().rss / 1024 ** 2)\n",
    "    memorys.append(resource.getrusage(resource.RUSAGE_SELF).ru_maxrss/ 1024 ** 2)\n",
    "    et=Timer()\n",
    "    et.start()\n",
    "    mut_idx=[]\n",
    "    for i in range(num_features):\n",
    "        t=Timer()\n",
    "        t.start()\n",
    "        ind=ind_all[i]\n",
    "        model = SelectKBest(mutual_info_classif, k=(i+1)).fit(X[ind],Y[ind])\n",
    "        tmp=model.get_support(indices=True).tolist()\n",
    "        mut_idx.append(tmp)\n",
    "        model=SVM(X[:,tmp],Y,class_weight=class_weight)\n",
    "        train_error.append(get_error(model,X[:,tmp],Y,sample_weight=train_sample_weight))\n",
    "        train_acc.append(model.score(X[:,tmp],Y,sample_weight=train_sample_weight))\n",
    "        test_error.append(get_error(model,X_test[:,tmp],Y_test,sample_weight=test_sample_weight))\n",
    "        test_acc.append(model.score(X_test[:,tmp],Y_test,sample_weight=test_sample_weight))\n",
    "        step_times.append(t.stop())\n",
    "    elapsed_times.append(et.stop())\n",
    "    print('mut_idx')\n",
    "    \n",
    "    # chi2\n",
    "    memorys.append(psutil.Process(os.getpid()).memory_info().rss / 1024 ** 2)\n",
    "    memorys.append(resource.getrusage(resource.RUSAGE_SELF).ru_maxrss/ 1024 ** 2)\n",
    "    et=Timer()\n",
    "    et.start()\n",
    "    chi_idx=[]\n",
    "    for i in range(num_features):\n",
    "        t=Timer()\n",
    "        t.start()\n",
    "        ind=ind_all[i]\n",
    "        model = SelectKBest(chi2, k=(i+1)).fit(X[ind],Y[ind])\n",
    "        tmp=model.get_support(indices=True).tolist()\n",
    "        chi_idx.append(tmp)\n",
    "        model=SVM(X[:,tmp],Y,class_weight=class_weight)\n",
    "        train_error.append(get_error(model,X[:,tmp],Y,sample_weight=train_sample_weight))\n",
    "        train_acc.append(model.score(X[:,tmp],Y,sample_weight=train_sample_weight))\n",
    "        test_error.append(get_error(model,X_test[:,tmp],Y_test,sample_weight=test_sample_weight))\n",
    "        test_acc.append(model.score(X_test[:,tmp],Y_test,sample_weight=test_sample_weight))\n",
    "        step_times.append(t.stop())\n",
    "    elapsed_times.append(et.stop())\n",
    "    print('chi_idx')\n",
    "\n",
    "    # tree\n",
    "    memorys.append(psutil.Process(os.getpid()).memory_info().rss / 1024 ** 2)\n",
    "    memorys.append(resource.getrusage(resource.RUSAGE_SELF).ru_maxrss/ 1024 ** 2)\n",
    "    et=Timer()\n",
    "    et.start()\n",
    "    t=Timer()\n",
    "    t.start()\n",
    "    tree_idx=[]\n",
    "    for i in range(num_features):\n",
    "        t=Timer()\n",
    "        t.start()\n",
    "        ind=ind_all[i]\n",
    "        #model = ExtraTreesClassifier(criterion=para['criterion'],min_samples_leaf=para['min_samples_leaf']).fit(X[ind],Y[ind])\n",
    "        model = ExtraTreesClassifier(min_samples_leaf=10).fit(X[ind],Y[ind])\n",
    "        coef = [float('-inf') if math.isnan(x) or x==float('inf') else x for x in np.abs(model.feature_importances_)]\n",
    "        tmp=sorted(range(len(coef)), key=lambda i: coef[i], reverse=True)[:(i+1)]\n",
    "        tree_idx.append(tmp)\n",
    "        model=SVM(X[:,tmp],Y,class_weight=class_weight)\n",
    "        train_error.append(get_error(model,X[:,tmp],Y,sample_weight=train_sample_weight))\n",
    "        train_acc.append(model.score(X[:,tmp],Y,sample_weight=train_sample_weight))\n",
    "        test_error.append(get_error(model,X_test[:,tmp],Y_test,sample_weight=test_sample_weight))\n",
    "        test_acc.append(model.score(X_test[:,tmp],Y_test,sample_weight=test_sample_weight))\n",
    "        step_times.append(t.stop())\n",
    "    elapsed_times.append(et.stop())\n",
    "    print('tree_idx')\n",
    "    \n",
    "    # naive SVM\n",
    "    memorys.append(psutil.Process(os.getpid()).memory_info().rss / 1024 ** 2)\n",
    "    memorys.append(resource.getrusage(resource.RUSAGE_SELF).ru_maxrss/ 1024 ** 2)\n",
    "    et=Timer()\n",
    "    et.start()\n",
    "    t=Timer()\n",
    "    t.start()\n",
    "    svm_idx=[]\n",
    "    for i in range(num_features):\n",
    "        t=Timer()\n",
    "        t.start()\n",
    "        ind=ind_all[i]\n",
    "        model=svm.LinearSVC(loss='squared_hinge',tol=1,C=10,class_weight=class_weight, max_iter=10000)\n",
    "        model.fit(X[ind],Y[ind])\n",
    "        weight=np.sum(np.abs(model.coef_),axis=0).tolist()\n",
    "        coef = [0 if math.isnan(x) or x==float('inf') else x for x in weight]\n",
    "        tmp=sorted(range(len(coef)), key=lambda i: coef[i], reverse=True)[:(i+1)]\n",
    "        svm_idx.append(tmp)\n",
    "        model=SVM(X[:,tmp],Y,class_weight=class_weight)\n",
    "        train_error.append(get_error(model,X[:,tmp],Y,sample_weight=train_sample_weight))\n",
    "        train_acc.append(model.score(X[:,tmp],Y,sample_weight=train_sample_weight))\n",
    "        test_error.append(get_error(model,X_test[:,tmp],Y_test,sample_weight=test_sample_weight))\n",
    "        test_acc.append(model.score(X_test[:,tmp],Y_test,sample_weight=test_sample_weight))\n",
    "        step_times.append(t.stop())\n",
    "    elapsed_times.append(et.stop())\n",
    "    print('svm_idx')\n",
    "    \n",
    "    \n",
    "    memorys.append(psutil.Process(os.getpid()).memory_info().rss / 1024 ** 2)\n",
    "    memorys.append(resource.getrusage(resource.RUSAGE_SELF).ru_maxrss/ 1024 ** 2)\n",
    "    \n",
    "    text_create(path,'compare_train_error',train_error)\n",
    "    text_create(path,'compare_test_error',test_error)\n",
    "    text_create(path,'compare_train_acc',train_acc)\n",
    "    text_create(path,'compare_test_acc',test_acc)\n",
    "    text_create(path,'ran_idx',ran_idx)\n",
    "    text_create(path,'cor_idx',cor_idx)\n",
    "    text_create(path,'mut_idx',mut_idx)\n",
    "    text_create(path,'chi_idx',chi_idx)\n",
    "    text_create(path,'tree_idx',tree_idx)\n",
    "    text_create(path,'svm_idx',svm_idx)\n",
    "    text_create(path,'ind_all',ind_all)\n",
    "    text_create(path,'compare_step_times',step_times)\n",
    "    text_create(path,'compare_elapsed_times',elapsed_times)\n",
    "    text_create(path,'compare_memorys',memorys)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "northern-print",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "psychological-wallpaper",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(913, 10000)\n",
      "<class 'numpy.ndarray'>\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "(913, 10000) (913,) 7\n",
      "(730, 10000)\n",
      "(183, 10000)\n",
      "0 0: 118\n",
      "1 1: 108\n",
      "2 2: 90\n",
      "3 3: 106\n",
      "4 4: 101\n",
      "5 5: 281\n",
      "6 6: 109\n"
     ]
    }
   ],
   "source": [
    "label_df=pd.read_csv('./data/cortex_svz_cellcentroids.csv')\n",
    "target=label_df['Field of View'].values\n",
    "raw_data = np.genfromtxt('./data/cortex_svz_counts_cell_types.csv', delimiter=',').transpose()\n",
    "raw_data[0,0]=3\n",
    "print(raw_data.shape)\n",
    "classes=np.unique(target).tolist()\n",
    "keys=classes\n",
    "gene=pd.read_csv('./data/genes.csv',header=None)[0].values\n",
    "\n",
    "x=label_df['X'].values\n",
    "y=label_df['Y'].values\n",
    "\n",
    "nonzero_row_indice, _ = raw_data.nonzero()\n",
    "unique_nonzero_indice = np.unique(nonzero_row_indice)\n",
    "raw_data=raw_data[unique_nonzero_indice]\n",
    "target=target[unique_nonzero_indice]\n",
    "raw_data=raw_data.transpose()\n",
    "nonzero_row_indice, _ = raw_data.nonzero()\n",
    "unique_nonzero_indice = np.unique(nonzero_row_indice)\n",
    "raw_data=raw_data[unique_nonzero_indice]\n",
    "gene=gene[unique_nonzero_indice]\n",
    "\n",
    "data=normalize(raw_data.transpose(),axis=1, norm='l2')\n",
    "\n",
    "idx = np.arange(np.shape(data)[0])\n",
    "random.shuffle(idx)\n",
    "X_train = data[idx[:int(np.shape(data)[0]*4/5)],:]\n",
    "y_train = target[idx[:int(np.shape(data)[0]*4/5)]]\n",
    "X_test = data[idx[int(np.shape(data)[0]*4/5):],:]\n",
    "y_test = target[idx[int(np.shape(data)[0]*4/5):]]\n",
    "\n",
    "del label_df,raw_data,nonzero_row_indice,unique_nonzero_indice\n",
    "\n",
    "print(type(data))\n",
    "print(keys)\n",
    "print(np.shape(data),np.shape(target),len(np.unique(target)))\n",
    "print(np.shape(X_train))\n",
    "print(np.shape(X_test))\n",
    "for i in np.unique(target):\n",
    "    print(str(i)+' '+ str(keys[i])+': '+str(np.count_nonzero((target==i)*1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "consolidated-profession",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creation of the directory results failed\n",
      "Successfully created the directory results/compare_samecell \n",
      "ran_idx\n",
      "cor_idx\n",
      "mut_idx\n",
      "chi_idx\n",
      "tree_idx\n",
      "svm_idx\n"
     ]
    }
   ],
   "source": [
    "memorys=[]\n",
    "memorys.append(psutil.Process(os.getpid()).memory_info().rss / 1024 ** 2)\n",
    "memorys.append(resource.getrusage(resource.RUSAGE_SELF).ru_maxrss/ 1024 ** 2)\n",
    "\n",
    "'''\n",
    "Parameters\n",
    "'''\n",
    "num_features = 30\n",
    "init_samples=10\n",
    "num_samples=10\n",
    "\n",
    "path='results/compare_samecell'\n",
    "\n",
    "try:\n",
    "    os.mkdir('results')\n",
    "except OSError:\n",
    "    print (\"Creation of the directory %s failed\" % 'results')\n",
    "else:\n",
    "    print (\"Successfully created the directory %s \" % 'results')\n",
    "try:\n",
    "    os.mkdir(path)\n",
    "except OSError:\n",
    "    print (\"Creation of the directory %s failed\" % path)\n",
    "else:\n",
    "    print (\"Successfully created the directory %s \" % path)\n",
    "    \n",
    "\n",
    "compare_mincom_samples(path, memorys, X_train, y_train, X_test,y_test,num_features,init_samples=init_samples,\n",
    "                       num_samples=num_samples,class_weight=None)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convinced-journey",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
